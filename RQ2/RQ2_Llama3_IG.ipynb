{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Llama 3 Explanations with Captum Integrated Gradients\n",
        "\n",
        "This notebook computes token-level attributions for **Llama 3** toxic completions using **Captum Integrated Gradients**.\n",
        "\n",
        "We attribute the **log-probability of the actual completion tokens** (given the prompt) back to all input tokens.\n",
        "The outputs are compatible with the JSON structure used in RQ2 for Gemma and Mistral.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "import torch\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(\"WARNING: No GPU detected. Please enable GPU in Runtime > Change runtime type > GPU\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install -q transformers bitsandbytes accelerate huggingface_hub captum\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Hugging Face Authentication\n",
        "\n",
        "Required for accessing Llama 3. Get your token from https://huggingface.co/settings/tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "# Run this cell and enter your Hugging Face token when prompted\n",
        "login()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load Data (toxic prompts and Llama 3 completions)\n",
        "\n",
        "We assume the following RQ1 files are available:\n",
        "- `RQ1/toxic.jsonl`\n",
        "- `RQ1/completions_scores_llama3.jsonl`\n",
        "\n",
        "Upload or copy them into this Colab environment before running the next cells.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# (Optional) create RQ1 directory if needed\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "os.makedirs(\"RQ1\", exist_ok=True)\n",
        "print(\"Ensured RQ1 directory exists\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Option 1: Use Colab's file upload widget to upload RQ1 files\n",
        "from google.colab import files\n",
        "\n",
        "print(\"Upload toxic.jsonl and completions_scores_llama3.jsonl from RQ1:\")\n",
        "uploaded = files.upload()\n",
        "for filename in uploaded.keys():\n",
        "    target = filename\n",
        "    if not filename.startswith(\"RQ1/\"):\n",
        "        target = f\"RQ1/{filename}\"\n",
        "    os.makedirs(os.path.dirname(target), exist_ok=True)\n",
        "    os.replace(filename, target)\n",
        "    print(f\"Saved: {target}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify files are present\n",
        "required_files = [\n",
        "    \"RQ1/toxic.jsonl\",\n",
        "    \"RQ1/completions_scores_llama3.jsonl\",\n",
        "]\n",
        "\n",
        "for f in required_files:\n",
        "    if os.path.exists(f):\n",
        "        size_kb = os.path.getsize(f) / 1024\n",
        "        print(f\"✓ {f} ({size_kb:.1f} KB)\")\n",
        "    else:\n",
        "        print(f\"✗ {f} MISSING\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "TOXICITY_THRESHOLD = 0.7\n",
        "\n",
        "def load_prompts(prompts_file: str):\n",
        "    prompts = []\n",
        "    with open(prompts_file, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            if line.strip():\n",
        "                prompts.append(json.loads(line)['prompt']['text'])\n",
        "    return prompts\n",
        "\n",
        "\n",
        "def load_completions_with_scores(scores_file: str):\n",
        "    completions, scores = [], []\n",
        "    with open(scores_file, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            if line.strip():\n",
        "                record = json.loads(line)\n",
        "                completions.append(record['completion'])\n",
        "                scores.append(record['score'])\n",
        "    return completions, scores\n",
        "\n",
        "\n",
        "def filter_toxic_outputs(prompts, completions, scores, threshold: float):\n",
        "    toxic_prompts, toxic_completions, toxic_scores = [], [], []\n",
        "    for prompt, completion, score in zip(prompts, completions, scores):\n",
        "        if score >= threshold:\n",
        "            toxic_prompts.append(prompt)\n",
        "            toxic_completions.append(completion)\n",
        "            toxic_scores.append(score)\n",
        "    return toxic_prompts, toxic_completions, toxic_scores\n",
        "\n",
        "prompts = load_prompts(\"RQ1/toxic.jsonl\")\n",
        "completions, scores = load_completions_with_scores(\"RQ1/completions_scores_llama3.jsonl\")\n",
        "\n",
        "print(f\"Loaded {len(prompts)} prompts and {len(completions)} completions\")\n",
        "\n",
        "toxic_prompts, toxic_completions, toxic_scores = filter_toxic_outputs(\n",
        "    prompts, completions, scores, TOXICITY_THRESHOLD\n",
        ")\n",
        "print(f\"Found {len(toxic_prompts)} toxic outputs (threshold = {TOXICITY_THRESHOLD})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Load Llama 3 model and tokenizer\n",
        "\n",
        "We load the causal LM and its tokenizer, using 8-bit quantization for memory efficiency where possible.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
        "\n",
        "print(f\"Loading tokenizer: {model_name}\")\n",
        "llama_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# Ensure we have a pad token for attention mask convenience\n",
        "if llama_tokenizer.pad_token is None:\n",
        "    llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
        "llama_tokenizer.padding_side = \"left\"\n",
        "\n",
        "print(\"Loading model (this may take a while)...\")\n",
        "if torch.cuda.is_available():\n",
        "    quant_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "    llama_model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        quantization_config=quant_config,\n",
        "        device_map=\"auto\",\n",
        "    )\n",
        "else:\n",
        "    llama_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "llama_model.eval()\n",
        "print(\"Model loaded.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Define Captum Integrated Gradients setup\n",
        "\n",
        "We define a forward function that returns the **sum of log-probabilities of the completion tokens** given the full sequence, and then use Captum Integrated Gradients on the input embeddings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from captum.attr import IntegratedGradients\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import gc\n",
        "\n",
        "llama_emb = llama_model.get_input_embeddings()\n",
        "\n",
        "\n",
        "def forward_completion_logprob(\n",
        "    inputs_embeds: torch.Tensor,\n",
        "    attention_mask: torch.Tensor,\n",
        "    completion_start_idx: int,\n",
        "    completion_token_ids: torch.Tensor,\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"Return scalar: sum log-probabilities of completion tokens.\n",
        "\n",
        "    inputs_embeds: [1, seq_len, hidden]\n",
        "    attention_mask: [1, seq_len]\n",
        "    completion_start_idx: index in the sequence where completion tokens begin\n",
        "    completion_token_ids: [completion_len]\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        pass  # just to make intent explicit; Captum will handle gradients\n",
        "\n",
        "    outputs = llama_model(inputs_embeds=inputs_embeds, attention_mask=attention_mask)\n",
        "    logits = outputs.logits  # [1, seq_len, vocab]\n",
        "\n",
        "    # Shift for causal LM: logits[:, t-1] predicts token at position t\n",
        "    # completion positions correspond to indices [completion_start_idx .. completion_start_idx+L-1]\n",
        "    start_for_logits = completion_start_idx - 1\n",
        "    end_for_logits = start_for_logits + completion_token_ids.shape[0]\n",
        "\n",
        "    # Safety clamp in case of boundary issues\n",
        "    start_for_logits = max(start_for_logits, 0)\n",
        "    end_for_logits = min(end_for_logits, logits.shape[1])\n",
        "\n",
        "    pred_logits = logits[:, start_for_logits:end_for_logits, :]  # [1, L, vocab]\n",
        "\n",
        "    # Adjust token ids slice if we had to clamp\n",
        "    effective_len = pred_logits.shape[1]\n",
        "    target_ids = completion_token_ids[:effective_len]\n",
        "\n",
        "    log_probs = F.log_softmax(pred_logits, dim=-1)  # [1, L, vocab]\n",
        "    idx = torch.arange(effective_len, device=log_probs.device)\n",
        "    selected_log_probs = log_probs[0, idx, target_ids]\n",
        "\n",
        "    # Sum of log-probs as scalar\n",
        "    score = selected_log_probs.sum()\n",
        "    return score\n",
        "\n",
        "\n",
        "ig = IntegratedGradients(forward_completion_logprob)\n",
        "print(\"Integrated Gradients object created.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Compute token attributions for toxic examples\n",
        "\n",
        "For each toxic prompt–completion pair, we:\n",
        "\n",
        "1. Tokenize the full text (`prompt + completion`).\n",
        "2. Identify the prompt / completion boundary.\n",
        "3. Run Integrated Gradients over the input embeddings.\n",
        "4. Aggregate attributions over the embedding dimension to get one score per token.\n",
        "5. Save results to `explanations_llama3_ig.json`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output_file = \"explanations_llama3_ig.json\"\n",
        "checkpoint_file = \"explanations_llama3_ig_checkpoint.json\"\n",
        "CHECKPOINT_INTERVAL = 20\n",
        "\n",
        "# Load checkpoint if it exists\n",
        "processed_indices = set()\n",
        "results = []\n",
        "if os.path.exists(checkpoint_file):\n",
        "    print(f\"Loading checkpoint from {checkpoint_file}\")\n",
        "    with open(checkpoint_file, \"r\", encoding=\"utf-8\") as f:\n",
        "        checkpoint = json.load(f)\n",
        "    processed_indices = set(checkpoint.get(\"processed_indices\", []))\n",
        "    results = checkpoint.get(\"results\", [])\n",
        "    print(f\"Resuming from {len(processed_indices)} processed items\")\n",
        "\n",
        "\n",
        "def save_checkpoint():\n",
        "    with open(checkpoint_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump({\n",
        "            \"processed_indices\": list(processed_indices),\n",
        "            \"results\": results,\n",
        "        }, f, indent=2, ensure_ascii=False)\n",
        "    print(f\"Checkpoint saved with {len(processed_indices)} items.\")\n",
        "\n",
        "\n",
        "for i, (prompt, completion) in enumerate(zip(toxic_prompts, toxic_completions)):\n",
        "    if i in processed_indices:\n",
        "        continue\n",
        "\n",
        "    if (i + 1) % 5 == 0:\n",
        "        print(f\"Processing {i + 1}/{len(toxic_prompts)}\")\n",
        "\n",
        "    try:\n",
        "        full_text = prompt + completion\n",
        "\n",
        "        # Tokenize full sequence\n",
        "        full_encoded = llama_tokenizer(\n",
        "            full_text,\n",
        "            return_tensors=\"pt\",\n",
        "            add_special_tokens=True,\n",
        "        )\n",
        "        input_ids = full_encoded[\"input_ids\"]  # [1, seq_len]\n",
        "        attention_mask = full_encoded[\"attention_mask\"]  # [1, seq_len]\n",
        "\n",
        "        # Tokenize prompt alone to find boundary\n",
        "        prompt_encoded = llama_tokenizer(\n",
        "            prompt,\n",
        "            return_tensors=\"pt\",\n",
        "            add_special_tokens=True,\n",
        "        )\n",
        "        prompt_token_ids = prompt_encoded[\"input_ids\"][0].tolist()\n",
        "        full_token_ids = input_ids[0].tolist()\n",
        "\n",
        "        # Default boundary: length of prompt token ids\n",
        "        prompt_end_idx = len(prompt_token_ids)\n",
        "        if prompt_token_ids == full_token_ids[:len(prompt_token_ids)]:\n",
        "            prompt_end_idx = len(prompt_token_ids)\n",
        "        else:\n",
        "            # Fallback: use naive boundary (could be refined if needed)\n",
        "            prompt_end_idx = len(prompt_token_ids)\n",
        "\n",
        "        # Completion token IDs (may be empty if something went wrong)\n",
        "        completion_token_ids = input_ids[0, prompt_end_idx:]\n",
        "        if completion_token_ids.numel() == 0:\n",
        "            print(f\"Warning: no completion tokens found for example {i}, skipping.\")\n",
        "            processed_indices.add(i)\n",
        "            continue\n",
        "\n",
        "        # Build inputs_embeds and baselines\n",
        "        inputs_embeds = llama_emb(input_ids).to(llama_model.device)\n",
        "        attention_mask = attention_mask.to(llama_model.device)\n",
        "        completion_token_ids = completion_token_ids.to(llama_model.device)\n",
        "\n",
        "        # Baseline: zero embeddings\n",
        "        baselines = torch.zeros_like(inputs_embeds)\n",
        "\n",
        "        # IG attribute\n",
        "        attributions = ig.attribute(\n",
        "            inputs=inputs_embeds,\n",
        "            baselines=baselines,\n",
        "            additional_forward_args=(\n",
        "                attention_mask,\n",
        "                int(prompt_end_idx),\n",
        "                completion_token_ids,\n",
        "            ),\n",
        "            n_steps=50,\n",
        "        )  # [1, seq_len, hidden]\n",
        "\n",
        "        # Aggregate over hidden dimension to get per-token scores\n",
        "        token_scores = attributions.squeeze(0).norm(dim=-1).detach().cpu().numpy().tolist()\n",
        "\n",
        "        # Decode tokens (string form) for analysis compatibility\n",
        "        tokens = llama_tokenizer.convert_ids_to_tokens(full_token_ids)\n",
        "\n",
        "        # Split into prompt / completion parts\n",
        "        prompt_tokens = tokens[:prompt_end_idx]\n",
        "        completion_tokens = tokens[prompt_end_idx:]\n",
        "        prompt_attributions = token_scores[:prompt_end_idx]\n",
        "        completion_attributions = token_scores[prompt_end_idx:]\n",
        "\n",
        "        results.append({\n",
        "            \"prompt_tokens\": prompt_tokens,\n",
        "            \"completion_tokens\": completion_tokens,\n",
        "            \"prompt_attributions\": prompt_attributions,\n",
        "            \"completion_attributions\": completion_attributions,\n",
        "        })\n",
        "        processed_indices.add(i)\n",
        "\n",
        "        if (i + 1) % CHECKPOINT_INTERVAL == 0:\n",
        "            save_checkpoint()\n",
        "            gc.collect()\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing item {i + 1}: {e}\")\n",
        "        results.append({\n",
        "            \"prompt\": prompt,\n",
        "            \"completion\": completion,\n",
        "            \"error\": str(e),\n",
        "        })\n",
        "        processed_indices.add(i)\n",
        "        save_checkpoint()\n",
        "        gc.collect()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "# Save final results\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(results, f, indent=2, ensure_ascii=False)\n",
        "print(f\"Saved final results to {output_file} with {len(results)} entries.\")\n",
        "\n",
        "# Cleanup checkpoint if everything completed\n",
        "if os.path.exists(checkpoint_file):\n",
        "    os.remove(checkpoint_file)\n",
        "    print(\"Removed checkpoint file after successful completion.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "if os.path.exists(output_file):\n",
        "    print(f\"Downloading {output_file}...\")\n",
        "    files.download(output_file)\n",
        "else:\n",
        "    print(f\"{output_file} not found.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
